{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148f0c23-8f2f-48a7-8359-8604cb320f01",
   "metadata": {},
   "source": [
    "# Local RAG with Gemma2 and Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4e7be-9e0a-4f4c-955e-df2baeb53e63",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606d131-9c34-4b1b-9684-0ea256f01264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import tempfile\n",
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, JSONLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d2d75-7acc-4f1f-82f2-48cae3dd2cfa",
   "metadata": {},
   "source": [
    "## Data Ingestion section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb75c2-f61a-4f3a-823b-19382d997200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the file, split it into chunks, and add them to the vector store\n",
    "def add_to_vector_store(file, vector_store, chunk_size=1000, chunk_overlap=200):\n",
    "    if file:\n",
    "        # Use tempfile because Langchain Loaders only accept a file_path\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "            tmp.write(file.getvalue())\n",
    "            tmp_file_path = tmp.name\n",
    "\n",
    "        # Use Langchain Loaders to load the file into a Document object (which stores page content and metadata)\n",
    "        if file.type == \"application/pdf\":\n",
    "            loader = PyPDFLoader(file_path = tmp_file_path)\n",
    "        elif file.type == \"application/json\":\n",
    "            loader = JSONLoader(file_path = tmp_file_path, jq_schema=\".\", text_content=False)\n",
    "        elif file.type == \"text/markdown\":\n",
    "            loader = UnstructuredMarkdownLoader(file_path = tmp_file_path)        \n",
    "        else:\n",
    "            loader = TextLoader(file_path = tmp_file_path)\n",
    "\n",
    "        data = loader.load()\n",
    "\n",
    "        # Replace temporary file name with original file name in documents' metadata\n",
    "        for document in data:\n",
    "            document.metadata[\"source\"] = file.name\n",
    "\n",
    "        print(f\"Loaded {len(data)} documents from {file.name}\")\n",
    "        # Use Langchain Text Splitter to chunk the document into smaller pieces\n",
    "        # From LangChain Docs (https://python.langchain.com/docs/how_to/recursive_text_splitter/):\n",
    "        # This text splitter is the recommended one for generic text. \n",
    "        # It is parameterized by a list of characters. It tries to split on them in order until \n",
    "        # the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. \n",
    "        # This has the effect of trying to keep all paragraphs (and then sentences, and then words) \n",
    "        # together as long as possible, as those would generically seem to be the strongest semantically \n",
    "        # related pieces of text.\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "                                                  chunk_overlap=chunk_overlap,\n",
    "                                                  add_start_index=True,  # track index in original document\n",
    "                                                )\n",
    "        chunked_data = splitter.split_documents(data)\n",
    "        \n",
    "        print(f\"Chunked {file.name} into {len(chunked_data)} pieces\")\n",
    "\n",
    "        # Upload the chunked data to the ChromaDB collection\n",
    "        uuids = [file.name + str(uuid4()) for _ in range(len(chunked_data))]\n",
    "        vector_store.add_documents(documents=chunked_data, ids=uuids)\n",
    "\n",
    "        print(f\"Uploaded {file.name} to ChromaDB\")\n",
    "        \n",
    "        # Delete the temporary file\n",
    "        tmp.close()\n",
    "        os.unlink(tmp_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94fcab3-59a2-4d69-9d6a-59052635dc43",
   "metadata": {},
   "source": [
    "## Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a7f35-f10b-4299-a7c5-92a8becea61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rewrite the user's query based on recent conversation history\n",
    "def rewrite_query(user_query, llm, conversation_history):\n",
    "\n",
    "    # Get the last two messages from the conversation history\n",
    "    context = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in conversation_history[-2:]])\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\",\"You are a helpful assistant that rewrites user query.\"),\n",
    "        (\"human\", \"\"\"Rewrite the following user query by incorporating relevant context from the last two messages of the conversation history.\n",
    "The rewritten query should:\n",
    "\n",
    "- Preserve the core intent and meaning of the original query\n",
    "- Avoid introducing new topics or queries that deviate from the original query\n",
    "- Be concise and clear, without any unnecessary information or repetition\n",
    "- Keep the same tone and style as the original query\n",
    "- DONT EVER ANSWER the Original query, but instead focus on rephrasing and expanding it into a new query\n",
    "- Return the output as plain text, without any additional formatting\n",
    "\n",
    "Return ONLY the rewritten query text, without any additional formatting or explanations.\n",
    "\n",
    "Conversation History:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Original query: \n",
    "```\n",
    "{user_query}\n",
    "```\n",
    "\n",
    "Rewritten query:\n",
    "\"\"\"\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "    ai_message = chain.invoke(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"user_query\": user_query,\n",
    "        }\n",
    "    )   \n",
    "\n",
    "    rewritten_query = ai_message.content.strip()\n",
    "\n",
    "    print(\"Original query:\", user_query)\n",
    "    print(\"Rewritten query:\", rewritten_query)\n",
    "\n",
    "    return rewritten_query\n",
    "\n",
    "# Function to handle the user input submission\n",
    "def chat(user_query, llm, retriever, conversation_history):   \n",
    "    # Rewrite the user's query based on the conversation history\n",
    "    if len(conversation_history) > 1:\n",
    "        rewritten_query = rewrite_query(user_query, llm, conversation_history)\n",
    "    else:\n",
    "        rewritten_query = user_query\n",
    "        \n",
    "    # Retrieve relevant context for the rewritten query from the vector database\n",
    "    retrieved_documents = retriever.invoke(rewritten_query)\n",
    "\n",
    "    print(\"Number of retrieved documents:\", len(retrieved_documents))\n",
    "\n",
    "    # Extract the text content of the retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_documents])\n",
    "\n",
    "    print(\"\\n Retrieved context: ```\", context, \"```\")\n",
    "\n",
    "    # Create a list of LangChain messages from the conversation history (limit to last 4 messages - starts with human message, ends with AI message)\n",
    "    messages = [HumanMessage(msg['content']) if msg['role'] == 'user' else AIMessage(msg['content']) for msg in conversation_history[-4:]]\n",
    "    \n",
    "    \n",
    "    # Add system message and human message \n",
    "    messages.insert(0, SystemMessage(\"Answer the following user query using the retrieved context. Provide a concise and informative answer that directly addresses the user's question. Use a maximum of three sentences to answer the question.\"))\n",
    "    messages.append(HumanMessage(f\"\"\"Question: \n",
    "```\n",
    "{user_query}\n",
    "```\n",
    "\n",
    "Context:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "))  \n",
    "\n",
    "    print(\"\\nMessages:\", messages)\n",
    "\n",
    "    # Generate the response from the model\n",
    "    return llm.stream(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960fcb0-8ee5-4282-9cc8-e812d7ed388c",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23eef65-1df0-4ab7-bf8f-ac317f03b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing...\")\n",
    "\n",
    "# Initialize session state for uploaded files, model, top_k and messages\n",
    "if 'uploaded_files' not in st.session_state:\n",
    "    st.session_state['uploaded_files'] = []\n",
    "if 'model' not in st.session_state:\n",
    "    st.session_state['model'] = \"gemma2:2b\"\n",
    "if 'top_k' not in st.session_state:\n",
    "    st.session_state['top_k'] = 3  \n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state['messages'] = []\n",
    "\n",
    "# Initialize Chat Ollama model\n",
    "llm = ChatOllama(\n",
    "    model = st.session_state[\"model\"],\n",
    "    temperature = 0.8\n",
    ")\n",
    "\n",
    "# Initialize Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    ")\n",
    "\n",
    "# Initialize chromadb \n",
    "vector_store = Chroma(\n",
    "    collection_name=\"vault\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",\n",
    ")\n",
    "\n",
    "# Use the vector store as a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\", \n",
    "        search_kwargs={\"k\": st.session_state['top_k']}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98365dc-db46-4d07-92ae-298607bdb7c4",
   "metadata": {},
   "source": [
    "## Streamlit UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8480ca-c268-4834-a215-09e1583a1efd",
   "metadata": {},
   "source": [
    "### Display Chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3f53e-fb67-4863-951a-6c626acc0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Vault App\")\n",
    "st.markdown(\"Welcome to the Vault App! Upload a file and ask a question to retrieve relevant context from the uploaded documents.\")\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743d6c8-c4d4-428a-a47a-25c0504309d5",
   "metadata": {},
   "source": [
    "### Sidebar for settings and File uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3593dbc-ddbd-4137-b78d-b47be0a21706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files\n",
    "st.sidebar.header(\"Upload a file\")\n",
    "uploaded_files = st.sidebar.file_uploader(\"Choose a file\", \n",
    "                                          type=[\"pdf\", \"txt\", \"json\", \"md\"],\n",
    "                                          accept_multiple_files=True)\n",
    "\n",
    "# If files have not been loaded into the ChromaDB collection, load them\n",
    "if uploaded_files:\n",
    "    new_files = [file for file in uploaded_files if file not in st.session_state.uploaded_files]\n",
    "    if new_files:\n",
    "        for new_file in new_files:\n",
    "            add_to_vector_store(new_file, vector_store)\n",
    "        st.session_state.uploaded_files.extend(new_files)\n",
    "\n",
    "# Settings\n",
    "st.sidebar.header(\"Settings\")\n",
    "st.session_state[\"model\"] = st.sidebar.selectbox(\"Select Model\", [\"gemma2:2b\", \"gemma2\"], index=0) # Model to use\n",
    "st.session_state[\"top_k\"] = st.sidebar.slider(\"Top K Context\", 1, 5, value=st.session_state.top_k)  # Top K context to retrieve\n",
    "\n",
    "# Toggle to reset conversation\n",
    "st.sidebar.button(\"Reset Conversation\", on_click= lambda: st.session_state.update(messages=[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ce8ed-0c51-461c-bef5-7211e02a4681",
   "metadata": {},
   "source": [
    "### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075e1b2-dc23-4f8f-a8b4-1ea36d9c14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the user_query is not None, \n",
    "if user_query := st.chat_input(\"Enter your message\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    # Display user message in chat message container\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_query)\n",
    "    \n",
    "    # Display assistant response in chat message container\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        stream = chat(user_query = user_query, \n",
    "                    llm = llm, \n",
    "                    retriever = retriever,\n",
    "                    conversation_history = st.session_state['messages'][:-1:])\n",
    "        \n",
    "        response = st.write_stream(stream)\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
