{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148f0c23-8f2f-48a7-8359-8604cb320f01",
   "metadata": {},
   "source": [
    "# Local RAG with Gemma2 and Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4e7be-9e0a-4f4c-955e-df2baeb53e63",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606d131-9c34-4b1b-9684-0ea256f01264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import tempfile\n",
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader, JSONLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d2d75-7acc-4f1f-82f2-48cae3dd2cfa",
   "metadata": {},
   "source": [
    "## Data Ingestion section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb75c2-f61a-4f3a-823b-19382d997200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the file, split it into chunks, and add them to the vector store\n",
    "def add_to_vector_store(file, vector_store, chunk_size=1000, chunk_overlap=200):\n",
    "    if file:\n",
    "        # Use tempfile because Langchain Loaders only accept a file_path\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "            tmp.write(file.getvalue())\n",
    "            tmp_file_path = tmp.name\n",
    "\n",
    "        # Use Langchain Loaders to load the file into a Document object (which stores page content and metadata)\n",
    "        if file.type == \"application/pdf\":\n",
    "            loader = PyPDFLoader(file_path = tmp_file_path)\n",
    "        elif file.type == \"application/json\":\n",
    "            loader = JSONLoader(file_path = tmp_file_path, jq_schema=\".\", text_content=False)\n",
    "        elif file.type == \"text/markdown\":\n",
    "            loader = UnstructuredMarkdownLoader(file_path = tmp_file_path)        \n",
    "        else:\n",
    "            loader = TextLoader(file_path = tmp_file_path)\n",
    "\n",
    "        data = loader.load()\n",
    "\n",
    "        # Replace temporary file name with original file name in documents' metadata\n",
    "        for document in data:\n",
    "            document.metadata[\"source\"] = file.name\n",
    "\n",
    "        print(f\"Loaded {len(data)} documents from {file.name}\")\n",
    "        # Use Langchain Text Splitter to chunk the document into smaller pieces\n",
    "        # From LangChain Docs (https://python.langchain.com/docs/how_to/recursive_text_splitter/):\n",
    "        # This text splitter is the recommended one for generic text. \n",
    "        # It is parameterized by a list of characters. It tries to split on them in order until \n",
    "        # the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. \n",
    "        # This has the effect of trying to keep all paragraphs (and then sentences, and then words) \n",
    "        # together as long as possible, as those would generically seem to be the strongest semantically \n",
    "        # related pieces of text.\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "                                                  chunk_overlap=chunk_overlap,\n",
    "                                                  add_start_index=True,  # track index in original document\n",
    "                                                )\n",
    "        chunked_data = splitter.split_documents(data)\n",
    "        \n",
    "        print(f\"Chunked {file.name} into {len(chunked_data)} pieces\")\n",
    "\n",
    "        # Upload the chunked data to the ChromaDB collection\n",
    "        uuids = [file.name + str(uuid4()) for _ in range(len(chunked_data))]\n",
    "        vector_store.add_documents(documents=chunked_data, ids=uuids)\n",
    "\n",
    "        print(f\"Uploaded {file.name} to ChromaDB\")\n",
    "        \n",
    "        # Delete the temporary file\n",
    "        tmp.close()\n",
    "        os.unlink(tmp_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
